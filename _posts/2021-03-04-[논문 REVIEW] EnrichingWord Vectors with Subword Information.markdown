---
title: "ë…¼ë¬¸ REVIEW - EnrichingWord Vectors with Subword Information"
layout: post
date: 2021-03-11
image: ../assets/images/markdown.jpg
headerImage: false
tag:
category: blog
author: yj
description: "ë…¼ë¬¸ REVIEW - EnrichingWord Vectors with Subword Information"
use_math: false
---

<br><br>
    
* ì›ë³¸ ë…¼ë¬¸ ë§í¬: https://arxiv.org/abs/1607.04606

<br><br>


## 1. Introduction

Continuous representations of words í•™ìŠµì— ê´€í•œ ì—°êµ¬ëŠ” 1988ë…„ë¶€í„° ì§€ì†ì ìœ¼ë¡œ ì´ì–´ì ¸ì™”ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ì „ ì—°êµ¬ë“¤ì€ parameter sharing ì—†ì´ ê° ë‹¨ì–´ë¥¼ distinct vectorë¡œ í‘œí˜„í•˜ì˜€ê³  ë‹¨ì–´ì˜ internal structureë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ morphologically rich languageì— ëŒ€í•´ì„œëŠ” ì¢‹ì€ word representation í•™ìŠµì´ ì´ë£¨ì–´ì§€ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤.

ë”°ë¼ì„œ í•´ë‹¹ ë…¼ë¬¸ì€ ë‹¤ìŒì„ í•˜ê³ ì í•©ë‹ˆë‹¤.

> In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors. Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information. We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.

<br>

## 2. Related Work

### 1) Morphological word representation

ìµœê·¼ì— word representationì— í˜•íƒœí•™ì  ì •ë³´ë¥¼ í¬í•¨í•˜ë ¤ëŠ” ë§ì€ ë°©ë²•ë“¤ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.
* Alexandrescu and Kirchhoff (2006): factored neural language models (words are represented as sets of features)
* Lazaridou et al. (2013), Luong et al. (2013), Botha and Blunsom (2014), Qiu et al. (2014): morphemes(í˜•íƒœì†Œ)ì—ì„œ representations of wordsê°€ íŒŒìƒë˜ë„ë¡ í•˜ëŠ” composition functions ì œì•ˆ

ì´ëŸ¬í•œ ë°©ì‹ë“¤ì€ ë‹¨ì–´ë“¤ì˜ í˜•íƒœì†Œ ë¶„ë¦¬ (morphological decomposition)ì— ì˜ì¡´í•˜ë‚˜ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ì‹ì€ ì´ì— ì˜ì¡´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

<br>

* SchÃ¼tze (1993): singular value decompositionì„ í†µí•´ representation of character four-grams í•™ìŠµ & four-grams representationsë¥¼ ë”í•´ì„œ representation of wordsë¥¼ ì–»ìŒ

ì´ëŠ” ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ì‹ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë°©ì‹ì…ë‹ˆë‹¤.


### 2) Character level features for NLP

ì´ëŠ” ë‹¨ì–´ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜ ë¬¸ì(character)ì—ì„œ ë°”ë¡œ language representationsë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
* Recurrent neural networks: language modeling, text normalization, part-of-speech tagging, parsing
* Convolutional neural networks: part-of-speech tagging, sentiment analysis, text classification, language modeling
* Sennrich et al. (2016), Luong and Manning (2016): machine translationì—ì„œ subword unitsì„ ì‚¬ìš©í•˜ì—¬ ë§ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì˜ representationì„ ì–»ëŠ” ë°©ì‹ì„ ì œì•ˆ

<br>

## 3. Model

ê°•ì˜ ë•Œ ì„¤ëª… ì˜ˆì •

<br>

## 4. Experimental setting / Results

### 1) Experimental setting

Cì˜ skipgram, cbow ëª¨ë¸ê³¼ ë¹„êµ <br>
Optimization
<ul>
    <li> Stochastic Gradient Descent </li>
    <li> training size = T </li>
    <li> # of passes over the data = P </li>
    <li> time = t </li>
    <li> ğ›¾_0ì´ fixed parameterì¼ ë•Œ step size = ğ›¾_0 (1âˆ’ğ‘¡/ğ‘‡ğ‘ƒ) </li>
</ul>
Implementation details
<ul>
    <li> word vectors dimension = 300 </li>
    <li> negative samples = 5 (for each positive example) </li>
    <li> context window size = c (uniformly sample the size c between 1 and 5) </li>
    <li> rejection threshold = 1e-4 </li>
    <li> keep the words that appear at least 5 times in the training set </li>
    <li> ğ›¾_0=0.025 (skipgram), ğ›¾_0=0.05 (cbow, our model) </li>
</ul>

### 2) Results

#### (1) Word similarity

Human judgementì™€ cosine similarity between the vector representationsì˜ Spearmanâ€™s rank correlation coefficientë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ training dataì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤(out-of-vocabulary)ì˜ representationì€ skipgramì´ë‚˜ cbowë¥¼ ì‚¬ìš©í•´ì„œëŠ” êµ¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ ë¹„êµë¥¼ ìœ„í•´ ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì—ëŠ” ê¸°ë³¸ ê°’ìœ¼ë¡œ null vectorsë¥¼ ë¶€ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ë•Œì˜ ëª¨ë¸ ì´ë¦„ì€ 'sisg -' ì…ë‹ˆë‹¤. fastTextë¥¼ ì‚¬ìš©í•˜ë©´ OOV ë‹¨ì–´ë“¤ì— ëŒ€í•´ì„œ ê° ë‹¨ì–´ì˜ n-gram vectorsë¥¼ ë”í•¨ìœ¼ë¡œì¨ valid representationì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œì˜ ëª¨ë¸ ì´ë¦„ì€ 'sisg (Subword Information Skip Gram)' ì…ë‹ˆë‹¤.

![result1](/assets/images/yj/210311/result1.PNG)

#### (2) Word analogies

We conducted evaluation on word analogy questions of the form A is to B as C is to D, where D must be predicted by the models

![result2](/assets/images/yj/210311/result2.PNG)

#### (3) Comparison with morphological representations

ë‹¤ìŒì˜ ì—°êµ¬ë“¤ê³¼ ì´ ë…¼ë¬¸ì˜ ì—°êµ¬ë¥¼ word similarity taskì— ëŒ€í•´ì„œ ë¹„êµí•´ë³´ì•˜ìŠµë‹ˆë‹¤. ë¹„êµí•˜ë ¤ëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•œ ë™ì¼í•œ datasetìœ¼ë¡œ ì´ ë…¼ë¬¸ì˜ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤.
<ul>
    <li> Luong et al. (2013) â€“ recursive neural network </li>
    <li> Oiu et al. (2014) â€“ morpheme cbow </li>
    <li> Soricut and Och (2015) â€“ morphological transformations </li>
    <li> Botha and Blunsom (2014) â€“ log-bilinear language model <li>
</ul>

![result3](/assets/images/yj/210311/result3.PNG)

#### (4) Effect of the size of the training data

Training data sizeë¥¼ ë°”ê¾¸ì–´ ê°€ë©° similarity taskì— ëŒ€í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ ë•Œ ë¹„êµ ëª¨ë¸ë¡œëŠ” cbow baselineì„ ì‚¬ìš©í•˜ì˜€ê³ , Wikipedia corpusë¥¼ ì²˜ìŒ 1, 2, 5, 10, 20, ê·¸ë¦¬ê³  50% ë§Œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. (no reshuffle of the dataset)

![result4](/assets/images/yj/210311/result4.PNG)

#### (5) Effect of the size of n-grams

ì•ì„œ Section 3.2 Subword modelsì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì´ ë…¼ë¬¸ì—ì„œëŠ” 3~6ê¹Œì§€ì˜ n-gramsë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

![result5](/assets/images/yj/210311/result5.PNG)

![result5_2](/assets/images/yj/210311/result5_2.PNG)

<br>

## 5. Qualitative analysis

### 1) Nearest neighbors

ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ëª¨ë¸ê³¼ skipgram baselineìœ¼ë¡œ í•™ìŠµëœ vertorsì™€ì˜ cosine similarityë¥¼ ì´ìš©í•˜ì—¬ nearest neighborsë¥¼ ì°¾ì•„ì„œ ë¹„êµí•´ë³´ì•˜ìŠµë‹ˆë‹¤. ê²°ê³¼ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ ë…¼ë¬¸ì˜ ëª¨ë¸ì´ ë³µì¡í•˜ê³  ê¸°ìˆ ì ì´ê³  ì˜ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì˜ nearest neighborsë¥¼ ë” ì˜ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

![qa1](/assets/images/yj/210311/qa1.PNG)

### 2) Character n-grams and morphemes

í•œ ë‹¨ì–´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ n-gramsê°€ í˜•íƒœì†Œì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í•´ë‹¹ ë‹¨ì–´(ì¦‰, n-gramsì˜ í•©)ì™€ í•´ë‹¹ ë‹¨ì–´ì˜ n-grams gì— ëŒ€í•´ gë¥¼ ì œê±°í•œ ê²ƒì˜ cosineì„ ë¹„êµí•˜ì—¬ ê·¸ ê°’ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ n-gramsì— ìˆœìœ„ë¥¼ ë§¤ê¹ë‹ˆë‹¤. ê·¸ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![qa2](/assets/images/yj/210311/qa2.PNG)

### 3) Word similarity for OOV words

ì´ ë…¼ë¬¸ì˜ ëª¨ë¸ì€ training setì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì— ëŒ€í•´ í•´ë‹¹ ë‹¨ì–´ë“¤ì˜ n-gramsì˜ í‰ê· ì„ ëƒ„ìœ¼ë¡œì¨ vector representationì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ representationsë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ training setì—ì„œ OOVì™€ ìœ ì‚¬í•œ ë‹¨ì–´ì˜ n-gramsì™€ OOVì˜ n-gramsì˜ cosine similarityë¥¼ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.

![qa3](/assets/images/yj/210311/qa3.PNG)

![qa3_2](/assets/images/yj/210311/qa3_2.PNG)

<br>

## 6. Conclusion

This paper investigate a simple method to learn word representations by taking into account subword information. This paper's approach incorporates character n-grams into the skipgram model. This model trains fast and does not require any preprocessing or supervision.
